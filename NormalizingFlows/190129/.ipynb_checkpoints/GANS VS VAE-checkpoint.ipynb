{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "- You can use NN libraries such as tensorflow, pytorch, etc. But implement GAN and VAE on your own.\n",
    "- Zero tolerance for plagiarism. Do not copy from Practice and Share; the student who submitted there originally only can use; Github tracks who pushed what code.\n",
    "- Total marks: 50\n",
    "- Marks will be for answering the questions asked below, not for the codes. Use plots, tables, etc. to convey your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CodingQuiz1lib as given\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import optimizers\n",
    "import time\n",
    "import scipy\n",
    "tfd = tfp.distributions\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAEs vs GANs\n",
    "The objective of this exercise is to compare VAEs and GANs for a generative modeling task (generating samples from a mixture of Gaussians).\n",
    "\n",
    "- Prepare the target probability distribution $p_x^*$ as a GMM with 5 components.\n",
    "(Using snippets from CodingQuiz1.py is allowed)\n",
    "- Construct and train a VAE to model $p^*_x$.\n",
    "- Construct and train a GAN to model $p^*_x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. Plot the samples from original $p^*_x$, VAE and GANs to compare them visually. (You may plot them on same or different plots) [10 marks]\n",
    "2. Quantify the performance of the two models and compare them. (Think what all metrics can you use - report as many as you can) [10 marks]\n",
    "3. Expressivity vs Efficiency: Which of the two models is more expressive? Which is more efficient? (Number of parameters vs performance, computations vs performance, etc.) [10 marks]\n",
    "4. Compare the pros and cons of the two models in a table. [10 marks]\n",
    "5. Do you find a structure in the latent space in the two models? E.g., each Gaussian component of $p^*_x$ may correspond to a specific region in the latent space. [10 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes and Answers\n",
    "It would be appreciated if you write modular codes and re-use them while answering different parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(given.Gaussian):\n",
    "    def __init__(self, mu=None, sigma=None):\n",
    "        given.Gaussian.__init__(self,mu,sigma)\n",
    "        self.scale=np.linalg.cholesky(self.sigma)\n",
    "        self.dist=tfd.MultivariateNormalTriL(loc=self.mu,scale_tril=self.scale)\n",
    "    def tf_sample(self, S=1):\n",
    "        return  tf.cast(self.dist.sample(S),\"float64\")\n",
    "    def tf_log_prob(self,u):\n",
    "        return tf.cast(self.dist.log_prob(u),\"float32\")\n",
    "    def tf_prob(self,u):\n",
    "        return self.dist.prob(u)\n",
    "    def one_tf_prob(self,u):\n",
    "        x=u-tf.transpose(self.mu)\n",
    "        x1=tf.linalg.matvec(self.sigmainv,x)\n",
    "        k=tf.tensordot(x1,x,1)\n",
    "        return tf.math.exp(-0.5*k)/self.Z\n",
    "    def my_tf_prob(self,u):  \n",
    "        return tf.map_fn(lambda t:self.one_tf_prob(t),elems=tf.cast(u,\"float64\"))\n",
    "    def my_tf_log_prob(self,u):\n",
    "        return tf.math.log(self.my_tf_prob(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM(given.GMM):\n",
    "    def __init__(self):\n",
    "        given.GMM.__init__(self)\n",
    "        self.scale=list()\n",
    "        self.mus=list()\n",
    "        for i in self.components:\n",
    "            self.scale.append(np.linalg.cholesky(i.sigma))\n",
    "            self.mus.append(i.mu)\n",
    "        self.dist=tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(probs=self.weights),\n",
    "                                        components_distribution=tfd.MultivariateNormalTriL(loc=self.mus,scale_tril=self.scale))\n",
    "        self.my_components=list()\n",
    "        for i in self.components:\n",
    "            self.my_components.append(Gaussian(mu=i.mu,sigma=i.sigma))\n",
    "        self.weights=tf.cast(self.weights,\"float64\")\n",
    "    def sample(self, S=1):\n",
    "        return  tf.cast(self.dist.sample(S),\"float64\")\n",
    "    def tf_log_prob(self,u):\n",
    "        return tf.cast(self.dist.log_prob(u),\"float32\")\n",
    "    def tf_prob(self,u):\n",
    "        return tf.cast(self.dist.prob(u),\"float32\")\n",
    "    def one_tf_prob(self,u):\n",
    "        p=0\n",
    "        for z, w in enumerate(self.weights):\n",
    "            p += w * self.my_components[z].one_tf_prob(u)\n",
    "        return p\n",
    "    def my_tf_prob(self,u):  \n",
    "        return tf.map_fn(lambda t:self.one_tf_prob(t),elems=tf.cast(u,\"float64\"))\n",
    "    def my_tf_log_prob(self,u):\n",
    "        return tf.math.log(self.my_tf_prob(u))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr=[.2,.3,.5]\n",
    "# u=[[10.0,5.0],[6.0,-5.0],[1.0,1.0]]\n",
    "# var=[ [[1.0,0.0],[0.0,1.0]], [[2.0,0.0],[0.0,2.0]] ,[[1.5,0.0],[0.0,1.0]]]\n",
    "# sd=[[1.0,1.0],[1.414,1.414],[1.5**.5,1]]\n",
    "# base_dist = tfp.distributions.MultivariateNormalDiag(loc=tf.zeros([2])) \n",
    "# mvgmm = tfd.MixtureSameFamily(\n",
    "#     mixture_distribution=tfd.Categorical(probs=pr),\n",
    "#     components_distribution=tfd.MultivariateNormalDiag(\n",
    "#         loc=u,\n",
    "#         scale_diag=sd)\n",
    "# )\n",
    "# nor1=tfd.MultivariateNormalDiag(loc=u[0],scale_diag=sd[0])\n",
    "# nor2=tfd.MultivariateNormalDiag(loc=u[1],scale_diag=sd[1])\n",
    "# nor3=tfd.MultivariateNormalDiag(loc=u[2],scale_diag=sd[2])\n",
    "# arr=[nor1,nor2,nor3]\n",
    "mvgmm=GMM()\n",
    "pr=mvgmm.weights\n",
    "arr=mvgmm.my_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000000\n",
    "data=tf.cast(mvgmm.sample(dataset_size),'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor=tfd.Normal(loc=0., scale=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "    \n",
    "    def __init__(self, dim_input = 2):\n",
    "        super().__init__(name='generator')\n",
    "        #layers\n",
    "        self.input_layer = keras.layers.Dense(units = dim_input)\n",
    "        self.layer1=layers.Dense(2, activation='relu')\n",
    "        self.layer2=layers.Dense(2,activation='relu')\n",
    "#         self.layer3=layers.Dense(16,activation='relu')\n",
    "        self.layer4=layers.Dense(2)\n",
    "        self.layer5=layers.Dense(2)\n",
    "    def Sample(self,z_mean, z_log_var):\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.reshape(nor.sample(batch*dim), shape=(batch, dim), name=None)\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def call(self, input_tensor,params=True):\n",
    "        x=self.input_layer(input_tensor)\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "#         x=self.layer3(x)\n",
    "        z_mean=self.layer4(x)\n",
    "        z_log_var=self.layer5(x)\n",
    "        if params:\n",
    "            return [z_mean,tf.exp(z_log_var)]\n",
    "        return  self.Sample(z_mean,z_log_var)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen=Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name = \"discriminator\")\n",
    "        self.input_layer = keras.layers.Dense(units = 2)\n",
    "        self.layer1=layers.Dense(8, activation='relu')\n",
    "        self.layer2=layers.Dense(8,activation='relu')\n",
    "        self.layer3=layers.Dense(2)\n",
    "        self.layer4=layers.Dense(2)\n",
    "    def call(self, input_tensor,params=True):\n",
    "        x = self.input_layer(input_tensor)\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "#         x=self.layer3(x)\n",
    "#         return(x)\n",
    "        z_mean=self.layer3(x)\n",
    "        z_log_var=self.layer4(x)\n",
    "        if params:\n",
    "            return [z_mean,tf.exp(z_log_var)]\n",
    "        return  self.Sample(z_mean,z_log_var)\n",
    "    def Sample(self,z_mean, z_log_var):\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.reshape(nor.sample(batch*dim), shape=(batch, dim), name=None)\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis=Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_KL_divergence(means,diag):\n",
    "    means_dot=tf.reduce_sum(tf.math.square(means),1)\n",
    "    trace=tf.reduce_sum(diag,1)\n",
    "    log_diag=tf.reduce_sum(tf.math.log(diag),1)\n",
    "    result=0.5 * (trace-log_diag+means_dot-1)\n",
    "    return result\n",
    "def cal_one_of_Expected_value(X,mean,sigma2):\n",
    "    log_trace=.5*tf.reduce_sum(tf.math.log(sigma2),1)\n",
    "    val=.5*tf.reduce_sum(tf.divide(tf.square(X-mean),sigma2),1)\n",
    "    return -(val+log_trace)\n",
    "def calculate_reconstruction_loss(x,x_):\n",
    "    return tf.math.sqrt(tf.reduce_sum(tf.square(x-x_),1))\n",
    "def calculate_loss(gen,dis,X):\n",
    "    zmeans,cov_diag=gen(X)\n",
    "#     print(\"mean and varirence\",zmeans,cov_diag)\n",
    "    ## calculated KLD \n",
    "    KLD=cal_KL_divergence(zmeans,cov_diag)\n",
    "    ### calculate E\n",
    "    val=0\n",
    "    for i in range(30):\n",
    "        sample=gen(X,False) ## got value of Z\n",
    "        xmean,x_diag=dis(sample) ## get u and sigma\n",
    "#         sampleX=dis(sample,False)\n",
    "        one_e=cal_one_of_Expected_value(X,xmean,x_diag)\n",
    "        val=val+one_e\n",
    "    val=val/30\n",
    "#     sample=gen(X,False) ## got value of Z\n",
    "#     x_=dis(sample,False)\n",
    "    error=-KLD+val\n",
    "#     print(val,KLD)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def tde(data,generator,discriminator,optimizer):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss=-tf.reduce_mean(calculate_loss(generator,discriminator,data))\n",
    "  gradients=tape.gradient(loss,tape.watched_variables())\n",
    "  optimizer.apply_gradients(zip(gradients,tape.watched_variables()))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_begin=time.time()\n",
    "epochs = 1\n",
    "optimizer = optimizers.Adam(learning_rate = 0.001)\n",
    "batch_size = 128\n",
    "batches_per_epoch = dataset_size//batch_size+1\n",
    "for i in range(epochs):\n",
    "    for j in range(batches_per_epoch):\n",
    "        data_batch = data[batch_size*j:batch_size*(j+1)]\n",
    "#       print(\"batch\",data_batch)\n",
    "        last_loss = tde(data_batch, gen, dis, optimizer)\n",
    "    print(last_loss)\n",
    "vae_end=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_Generator(keras.Model):\n",
    "    \n",
    "    def __init__(self, random_noise_size = 2):\n",
    "        super().__init__(name='generator')\n",
    "        #layers\n",
    "        self.input_layer = keras.layers.Dense(units = random_noise_size)\n",
    "        self.layer1=layers.Dense(16, activation='relu')\n",
    "        self.layer2=layers.Dense(16,activation='relu')\n",
    "        self.layer3=layers.Dense(2)\n",
    "\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        x=self.input_layer(input_tensor)\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        return  x\n",
    "    \n",
    "    def generate_noise(self,batch_size, random_noise_size):\n",
    "        return np.random.uniform(-1,1, size = (batch_size, random_noise_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_objective(dx_of_gx):\n",
    "    # Labels are true here because generator thinks he produces real images. \n",
    "    return cross_entropy(tf.ones_like(dx_of_gx), dx_of_gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GAN_Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = generator(np.random.uniform(-1,1, size =(1,2)))\n",
    "print(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name = \"discriminator\")\n",
    "        self.input_layer = keras.layers.Dense(units = 2)\n",
    "        self.layer1=layers.Dense(16, activation='relu')\n",
    "#         self.layer2=layers.Dense(16,activation='relu')\n",
    "        self.layer3=layers.Dense(1)\n",
    "\n",
    "        \n",
    "        self.logits = keras.layers.Dense(units = 1)  # This neuron tells us if the input is fake or real\n",
    "    def call(self, input_tensor):\n",
    "        x = self.input_layer(input_tensor)\n",
    "        x=self.layer1(x)\n",
    "#         x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_objective(d_x, g_z, smoothing_factor = 0.9):\n",
    "    \"\"\"\n",
    "    d_x = real output\n",
    "    g_z = fake output\n",
    "    \"\"\"\n",
    "    real_loss = cross_entropy(tf.ones_like(d_x) * smoothing_factor, d_x) # If we feed the discriminator with real images, we assume they all are the right pictures --> Because of that label == 1\n",
    "    fake_loss = cross_entropy(tf.zeros_like(g_z), g_z) # Each noise we feed in are fakes image --> Because of that labels are 0\n",
    "    \n",
    "    total_loss = tf.add(real_loss , fake_loss)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = GAN_Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = keras.optimizers.RMSprop()\n",
    "discriminator_optimizer = keras.optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def training_step(generator: Discriminator, discriminator: Discriminator, images:np.ndarray , k:int =1, batch_size = 32):\n",
    "    for _ in range(k):\n",
    "         with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            noise = generator.generate_noise(batch_size, 2)\n",
    "            g_z = generator(noise)\n",
    "            d_x_true = discriminator(images) # Trainable?\n",
    "            d_x_fake = discriminator(g_z) # dx_of_gx\n",
    "\n",
    "            discriminator_loss = discriminator_objective(d_x_true, d_x_fake)\n",
    "            # Adjusting Gradient of Discriminator\n",
    "            gradients_of_discriminator = disc_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "            discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables)) # Takes a list of gradient and variables pairs\n",
    "            \n",
    "              \n",
    "            generator_loss = generator_objective(d_x_fake)\n",
    "            # Adjusting Gradient of Generator\n",
    "            gradients_of_generator = gen_tape.gradient(generator_loss, generator.trainable_variables)\n",
    "            generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_begin=time.time()\n",
    "n=2\n",
    "epochs = 2\n",
    "batch_size = 100\n",
    "batches_per_epoch = dataset_size//batch_size+1\n",
    "for i in range(epochs):\n",
    "    for j in range(batches_per_epoch):\n",
    "        data_batch = data[batch_size*j:batch_size*(j+1)]\n",
    "        training_step(generator, discriminator, data_batch ,batch_size = batch_size , k = 1)\n",
    "gan_end=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Comparison of plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0],data[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution from VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p=tf.reshape(nor.sample(1000*2),(1000,2))\n",
    "X=dis(data_p,False)\n",
    "plt.scatter(X[:,0],X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution from GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list()\n",
    "y=list()\n",
    "for i in range(2000):\n",
    "    temp=np.random.uniform(-1,1, size = (1, 2))\n",
    "    x.append(generator(temp)[0])\n",
    "    y.append(temp[0])\n",
    "x=np.array(x)\n",
    "plt.scatter(x[:,0],x[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Latent space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_classification(x):\n",
    "    p=list()\n",
    "    for i in range(len(arr)):\n",
    "        p.append(arr[i].tf_prob(x)*pr[i])\n",
    "    tag=p.index(max(p)) \n",
    "    return tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For GANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepa=[[],[],[],[],[]]\n",
    "print(arr)\n",
    "for i in range(len(x)):\n",
    "    n=int(class_classification(tf.cast(x[i],'float64')))\n",
    "    sepa[n].append(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sepa[0])\n",
    "for i in range(len(arr)):\n",
    "    arr1=np.array(sepa[i])\n",
    "    if(len(arr1)>0):\n",
    "        plt.scatter(arr1[:,0],arr1[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepa1=[[],[],[],[],[]]\n",
    "for i in range(len(arr)):\n",
    "    samples=arr[i].sample(700)\n",
    "    sepa1[i]=dis(samples,False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(arr)):\n",
    "    arr1=np.array(sepa1[i])\n",
    "    plt.scatter(arr1[:,0],arr1[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(arr)):\n",
    "    arr1=np.array(sepa1[i])\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.scatter(arr1[:,0],arr1[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Metric for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FID \n",
    "- FID(x,g)=$|u_x-u_g|^2+tr(cov_x+cov_g-2(cov_xcov_g)^{1/2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_FID(x,g):\n",
    "    mean_x=np.mean(x,0)\n",
    "    mean_g=np.mean(g,0)\n",
    "    cov_x=np.cov(np.transpose(x))\n",
    "    cov_g=np.cov(np.transpose(g))\n",
    "    dif=np.sum(np.square(mean_x-mean_g))\n",
    "    fid=dif+np.trace(cov_x+cov_g-2*scipy.linalg.fractional_matrix_power(np.matmul(cov_x,cov_g),.5))\n",
    "    return fid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_FID(data[:10000,:],X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For GANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_FID(data[:10000,:],x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Expressivity Vs Effiency \n",
    "<table style=\"width:90%\">\n",
    "    <tr>\n",
    "        <th>Property</th>\n",
    "    <th>GAN</th>\n",
    "    <th>VAE</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>traning speed</td>\n",
    "    <td>Fast</td>\n",
    "    <td>Slow</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> number of parameters</td>\n",
    "    <td>Less</td>\n",
    "    <td>More</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> Exact Density</td>\n",
    "    <td>NO</td>\n",
    "    <td>YES</td>\n",
    "    </tr>\n",
    "    </table>\n",
    "    \n",
    "    \n",
    "In our case VAE performed better than GAN (it maybe because VAE assumes priors to be Gaussian and we are trying to Model Mixture of Gaussian hence accidental prior knowledge helped us ) but it usually seen that in image production GAN are far superior that VAE. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Expressive:-\n",
    " VAE are more expressive and they deal with probality distribution unlike GAN (more over a better visulization of latent space)\n",
    "#####  Effecient:-\n",
    " Usually Gans are said to be more efficient but due to the nature of our problem(Modeling GMM) VAE performed better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Comparison\n",
    "\n",
    "#### GANS\n",
    "<table style=\"width:90%\">\n",
    "    <tr>\n",
    "    <th>PROS</th>\n",
    "    <th>CONS</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Easy generation of similar looking Data.</td>\n",
    "        <td>Non-convergence: the model parameters oscillate, destabilize and never converge.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>GANs are unsupervised, so no labelled data is required to train them.</td>\n",
    "        <td>Mode collapse: the generator collapses which produces limited varieties of samples.Unlike VAE</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>GAN generate the sharpest images unlike VAE</td>\n",
    "        <td>Diminished gradient: the discriminator gets too successful that the generator gradient vanishes and learns nothing.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Not easy to visualize latent space </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "####  VAE\n",
    "<table style=\"width:90%\">\n",
    "<tr>\n",
    "<th>PROS</th>\n",
    "<th>CONS</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Gives the probality distribution advantage over GAN</td>\n",
    "<td>Not as good at generation compared to GAN</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>The ability to set complex priors in the latent is also nice especially in cases where you know something should make sense or you have a desired latent distribution.</td>\n",
    "<td>Slower to train compared to GAN</td>\n",
    "</tr>\n",
    "    <tr>\n",
    "<td>Easy and direct mapping to latent space.</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time required to train GAN \",-1*(gan_begin-gan_end))\n",
    "print(\"Time required to train VAE \",-1*(vae_begin-vae_end))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
